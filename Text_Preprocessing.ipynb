{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd785a42-6b9a-4ecc-965e-23484e4ca990",
   "metadata": {},
   "source": [
    "# Text Preprocessing \n",
    "### convert text features into numerical features ML models can work with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab09e0c-9e92-4e11-b488-be6d85c772e7",
   "metadata": {},
   "source": [
    "## 1. General text pre-processing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "234d8237-4b58-4c27-8137-3e4091577bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .  \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eab4f7e-e97e-4a5b-9cc5-893b7cae191c",
   "metadata": {},
   "source": [
    "### get rid of leading/trailing whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e116733a-ece8-42ac-ac80-6da3ad18ece2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a message to be cleaned. it may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .\n"
     ]
    }
   ],
   "source": [
    "text = text.strip().lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c69a9-bd58-419d-9b4b-d3bbddb0da0e",
   "metadata": {},
   "source": [
    "### Remove HTML tag, markups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c17f6f20-6333-4b01-bd0c-f3d71acc850d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This message be cleaned. It may involve some things like: , ?, :, '' adjacent spaces tabs . \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = re.compile('<.*?>').sub('', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7fcbbf-6304-48a9-8739-e9ce2b3adb14",
   "metadata": {},
   "source": [
    "### Replace punctuation with space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "580e7a77-2783-4623-a176-7e6511874804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This message be cleaned It may involve some things like     adjacent spaces tabs  \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "text = re.compile('[%s]' % re.escape(string.punctuation)).sub('', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84421e09-0814-4f3e-abba-aa3411da9530",
   "metadata": {},
   "source": [
    "### Remove extra spaces and tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f08f932f-bf8f-4333-bba2-75b27008f28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This message be cleaned It may involve some things like adjacent spaces tabs \n"
     ]
    }
   ],
   "source": [
    "text = re.sub('\\s+', ' ', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03891e45-4295-44fd-8954-56819e8c3cd6",
   "metadata": {},
   "source": [
    "## 2. Lexicon-based text pre-processing\n",
    "### normalize sentences in the dataset so that sentences are in a similar format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e1e31f-c3a4-4b5a-890d-ac9b7b9e7bb6",
   "metadata": {},
   "source": [
    "### Stop word removal\n",
    "#### remove words in our sentences that occur very frequently and don't contribute too much to the overall meaning of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66fb788b-ac15-4700-8828-d66a9b64c6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'message', 'be', 'cleaned', 'may', 'involve', 'some', 'things', 'like', 'adjacent', 'spaces', 'tabs', '']\n"
     ]
    }
   ],
   "source": [
    "stop_words = [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\"]\n",
    "words = text.lower().split(' ')\n",
    "filtered_sentence = []\n",
    "for w in words: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w)\n",
    "print(filtered_sentence)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7eedc69-e248-4bc6-a46c-fd2d293e71ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ac9cae3-f46c-4b9c-a09f-8a4ce2bea9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " message be cleaned may involve some things like adjacent spaces tabs \n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a15844-e323-464f-8bc5-e1b3db29e7c7",
   "metadata": {},
   "source": [
    "### Stemming: Stemming is a rule-based system to convert words into their root form.\n",
    "#### It removes suffixes from words. This helps us enhace similarities (if any) between sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f77dab3a-706b-4d05-a282-ae0ec441504c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 6.0 MB/s eta 0:00:00\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     ---------------------------------------- 298.0/298.0 KB ? eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\cinna\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ---------------------------------------- 78.5/78.5 KB 4.3 MB/s eta 0:00:00\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.6/96.6 KB ? eta 0:00:00\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.10.31-cp310-cp310-win_amd64.whl (267 kB)\n",
      "     ------------------------------------- 267.7/267.7 KB 16.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\cinna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.3 joblib-1.2.0 nltk-3.8.1 regex-2022.10.31 tqdm-4.64.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "beaf1771-683e-4daf-b3b5-75372b84821c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " messag be clean may involv some thing like adjac space tab \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "#Inialized the stemmer \n",
    "snow = SnowballStemmer('english')\n",
    "stemmed_sentence = []\n",
    "words = text.split(' ')\n",
    "for w in words: \n",
    "    stemmed_sentence.append(snow.stem(w))\n",
    "text = ' '.join(stemmed_sentence)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496c4c12-803c-4a85-a4a9-200a1ef055ab",
   "metadata": {},
   "source": [
    "## 3. Feature extraction - Bag of words\n",
    "### Steps:\n",
    "* Create vocabulary of known words\n",
    "* Measure presence of the known words in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31002dde-9e83-4f34-bb3e-3b54486e4d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1372d609-6d0f-40e6-8417-53f50bc30e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(binary = True)\n",
    "sentences = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "X = cv.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb4359b9-2828-40f6-bd66-ef02f1693360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 1 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"There are 9 unique words in the sentences above, the 9 words are indexed from 0 to 8 by the CountVectorizer, i.e. \"and\" is at 0, \"document\" is at 1, \"first\" is at 2. \n",
    "The array shows whether each sentence has an occurence of the word \n",
    "\"\"\"\n",
    "print(cv.vocabulary_)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd178a2a-7160-45bb-9a36-b94c90d184cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0 0 1]\n",
      " [0 0 0 1 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"When new words appear in the test set, it won't be counted by the CountVectorizer. The array will have the same length as the previous one. \n",
    "\"\"\"\n",
    "test_sentences = [\"this document has some new words\",\n",
    "                 \"this one is new too\"]\n",
    "X_test = cv.transform(test_sentences)\n",
    "print(X_test.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ebee19-9e37-499f-95d3-02211f0b3315",
   "metadata": {},
   "source": [
    "## 4. Full Example of text pre-processing steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f14d9a94-5cb1-4536-a8d1-c1af9904de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')\n",
    "stop_words = [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\"]\n",
    "\n",
    "#function that corries out all the general text pre-processing steps, lower case, strip leading/trailing whitespace, remove HTML tag, punctuation, and extra white space\n",
    "def preProcess(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub('', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text \n",
    "\n",
    "#function that carries out lexicon based text pre-processing steps, remove stop words, stemming\n",
    "def lexiconProcess(text, stop_words, stemmer):\n",
    "    filtered_sentence = []\n",
    "    words = text.split(' ')\n",
    "    for w in words: \n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(snowball.stem(w))\n",
    "    text = ' '.join(filtered_sentence)\n",
    "    return text \n",
    "    \n",
    "#function\n",
    "def cleanSentence(text, stop_words, stemmer):\n",
    "    return lexiconProcess(preProcess(text), stop_words, stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49e52d18-591c-49f0-956d-c026721a72d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4abddfa7-cfdb-47c1-abd2-60da69df0389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messag be clean may involv some thing like adjac space tab \n"
     ]
    }
   ],
   "source": [
    "print(cleanSentence(text, stop_words, snowball))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a82d2362-4f0f-400f-8de6-b6dad708439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare bag of words vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(binary = True, max_features = 50) #limit vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b6ba933-1b56-408b-a6d6-2885fc9412b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary \n",
      " {'like': 11, 'materi': 13, 'color': 4, 'overal': 19, 'how': 10, 'look': 12, 'work': 29, 'okay': 18, 'first': 7, 'two': 27, 'time': 26, 'use': 28, 'but': 3, 'third': 24, 'burn': 2, 'my': 15, 'face': 6, 'am': 1, 'not': 17, 'sure': 23, 'about': 0, 'product': 21, 'never': 16, 'thought': 25, 'would': 30, 'pay': 20, 'so': 22, 'much': 14, 'for': 8, 'hair': 9, 'dryer': 5}\n",
      "Bag of Words binary features: \n",
      " [[0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1]]\n",
      "(4, 31)\n"
     ]
    }
   ],
   "source": [
    "# Clean and vectorize a text feature with four samples\n",
    "text_feature = [\"I liked the material, color and overall how it looks.<br /><br />\",\n",
    "             \"Worked okay first two times I used it, but third time burned my face.\",\n",
    "             \"I am not sure about this product.\",\n",
    "             \"I never thought I would pay so much for a hair dryer.\",\n",
    "            ]\n",
    "\n",
    "\n",
    "#Apply text pre-processing steps\n",
    "text_clean = [cleanSentence(item, stop_words, snowball) for item in text_feature]\n",
    "\n",
    "#Apply text vectorization on the cleaned text\n",
    "\n",
    "text_vectorized = cv.fit_transform(text_clean)\n",
    "print('Vocabulary \\n', cv.vocabulary_)\n",
    "print('Bag of Words binary features: \\n', text_vectorized.toarray())\n",
    "\n",
    "print(text_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef2ac05-dc47-445f-b989-ecedbd1a617d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
